# Chinese-English Neural Machine Translation

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®å®ç°äº†åŸºäºRNNå’ŒTransformerçš„ä¸­è‹±æ–‡ç¥ç»æœºå™¨ç¿»è¯‘ç³»ç»Ÿï¼Œå¹¶å¯¹æ¯”äº†ä¸åŒæ¨¡å‹æ¶æ„å’Œè®­ç»ƒç­–ç•¥çš„æ€§èƒ½ã€‚

**ç›®æ ‡ï¼š**
- å®ç°RNN-based NMTï¼ˆä½¿ç”¨LSTMï¼Œæ”¯æŒå¤šç§æ³¨æ„åŠ›æœºåˆ¶ï¼‰
- å®ç°Transformer-based NMT
- å¯¹æ¯”ä¸åŒæ³¨æ„åŠ›æœºåˆ¶ã€è®­ç»ƒç­–ç•¥å’Œè§£ç ç­–ç•¥çš„æ•ˆæœ

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
final/
â”œâ”€â”€ README.md                           # æœ¬æ–‡æ¡£
â”œâ”€â”€ config.py                           # é…ç½®æ–‡ä»¶ï¼ˆæ¨¡å‹ã€è®­ç»ƒã€æ•°æ®å‚æ•°ï¼‰
â”œâ”€â”€ data_preprocess.py                  # æ•°æ®é¢„å¤„ç†ï¼ˆåˆ†è¯ã€è¯æ±‡è¡¨ã€DataLoaderï¼‰
â”‚
â”œâ”€â”€ models/                             # æ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ rnn_nmt.py                     # RNNæ¨¡å‹ï¼ˆEncoder-Decoder + Attentionï¼‰
â”‚   â””â”€â”€ transformer_nmt.py             # Transformeræ¨¡å‹
â”‚
â”œâ”€â”€ train_rnn.py                       # RNNè®­ç»ƒè„šæœ¬
â”œâ”€â”€ train_transformer.py               # Transformerè®­ç»ƒè„šæœ¬
â”œâ”€â”€ evaluate.py                        # RNNè¯„ä¼°è„šæœ¬
â”œâ”€â”€ evaluate_transformer.py            # Transformerè¯„ä¼°è„šæœ¬
â”‚
â”œâ”€â”€ run_attention_experiments.sh       # è¿è¡Œæ³¨æ„åŠ›æœºåˆ¶å¯¹æ¯”å®éªŒ
â”œâ”€â”€ run_attention_evaluations.sh       # è¯„ä¼°æ³¨æ„åŠ›æœºåˆ¶å®éªŒ
â”œâ”€â”€ generate_attention_report.py       # ç”Ÿæˆæ³¨æ„åŠ›æœºåˆ¶å¯¹æ¯”æŠ¥å‘Š
â”‚
â”œâ”€â”€ run_all_rnn_experiments.sh         # è¿è¡Œæ‰€æœ‰RNNå®éªŒ
â”œâ”€â”€ run_all_evaluations.sh             # è¯„ä¼°æ‰€æœ‰RNNå®éªŒ
â”œâ”€â”€ generate_report.py                 # ç”Ÿæˆå®Œæ•´å®éªŒæŠ¥å‘Š
â”‚
â”œâ”€â”€ AP0004_Midterm&Final_translation_dataset_zh_en/  # æ•°æ®é›†
â”‚   â”œâ”€â”€ train_10k.jsonl                # è®­ç»ƒé›†ï¼ˆ10kæ ·æœ¬ï¼‰
â”‚   â”œâ”€â”€ train_100k.jsonl               # è®­ç»ƒé›†ï¼ˆ100kæ ·æœ¬ï¼‰
â”‚   â”œâ”€â”€ valid.jsonl                    # éªŒè¯é›†
â”‚   â”œâ”€â”€ test.jsonl                     # æµ‹è¯•é›†
â”‚   â”œâ”€â”€ src_vocab.pkl                  # æºè¯­è¨€è¯æ±‡è¡¨
â”‚   â””â”€â”€ tgt_vocab.pkl                  # ç›®æ ‡è¯­è¨€è¯æ±‡è¡¨
â”‚
â”œâ”€â”€ checkpoints/                       # æ¨¡å‹æ£€æŸ¥ç‚¹
â”‚   â”œâ”€â”€ rnn_attention_dot/
â”‚   â”œâ”€â”€ rnn_attention_general/
â”‚   â”œâ”€â”€ rnn_attention_concat/
â”‚   â”œâ”€â”€ rnn_teacher_forcing/
â”‚   â”œâ”€â”€ rnn_free_running/
â”‚   â”œâ”€â”€ rnn_nmt/
â”‚   â””â”€â”€ transformer_nmt/
â”‚
â”œâ”€â”€ logs/                              # è®­ç»ƒæ—¥å¿—
â””â”€â”€ results/                           # è¯„ä¼°ç»“æœ
```

---

## ğŸ”§ ç¯å¢ƒé…ç½®

### ä¾èµ–é¡¹
```bash
torch>=2.0.0
jieba                  # ä¸­æ–‡åˆ†è¯
nltk                   # è‹±æ–‡åˆ†è¯å’ŒBLEUè¯„ä¼°
tqdm                   # è¿›åº¦æ¡
numpy
```

### GPUè®¾ç½®
æœ¬é¡¹ç›®ä½¿ç”¨ **CUDA:2**ï¼Œå¦‚éœ€ä¿®æ”¹è¯·ç¼–è¾‘ `config.py` ç¬¬98è¡Œï¼š
```python
device: str = 'cuda:2'  # æ”¹ä¸ºä½ çš„GPUç¼–å·
```

---

## ğŸ“š æ ¸å¿ƒæ–‡ä»¶è¯´æ˜

### 1. é…ç½®æ–‡ä»¶

#### `config.py`
å®šä¹‰æ‰€æœ‰å®éªŒçš„é…ç½®å‚æ•°ï¼š
- **DataConfig**: æ•°æ®è·¯å¾„ã€è¯æ±‡è¡¨å¤§å°ã€åºåˆ—é•¿åº¦
- **RNNModelConfig**: RNNæ¨¡å‹å‚æ•°ï¼ˆembed_dim, hidden_dim, attention_typeç­‰ï¼‰
- **TransformerModelConfig**: Transformeræ¨¡å‹å‚æ•°ï¼ˆd_model, n_heads, n_layersç­‰ï¼‰
- **TrainConfig**: è®­ç»ƒå‚æ•°ï¼ˆbatch_size, learning_rate, epochsç­‰ï¼‰
- **EvalConfig**: è¯„ä¼°å‚æ•°ï¼ˆdecode_method, beam_sizeç­‰ï¼‰

**é¢„å®šä¹‰å®éªŒé…ç½®ï¼š**
- `get_config('attention_dot')`: Dot-productæ³¨æ„åŠ›
- `get_config('attention_general')`: Generalæ³¨æ„åŠ›
- `get_config('attention_concat')`: Concatæ³¨æ„åŠ›
- `get_config('teacher_forcing')`: Teacher Forcing (TF=1.0)
- `get_config('free_running')`: Free Running (TF=0.0)
- `get_config('default')`: é»˜è®¤é…ç½®ï¼ˆLSTM + General Attention + TF=0.5ï¼‰

### 2. æ•°æ®å¤„ç†

#### `data_preprocess.py`
- **ChineseTokenizer**: ä½¿ç”¨Jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯
- **EnglishTokenizer**: ä½¿ç”¨NLTKè¿›è¡Œè‹±æ–‡åˆ†è¯
- **Vocabulary**: è¯æ±‡è¡¨ç±»ï¼ˆword2idx, idx2wordï¼‰
- **TranslationDataset**: PyTorch Dataset
- **prepare_data()**: ä¸€é”®å‡†å¤‡è®­ç»ƒ/éªŒè¯/æµ‹è¯•æ•°æ®

### 3. æ¨¡å‹å®šä¹‰

#### `models/rnn_nmt.py`
RNN-based Seq2Seqæ¨¡å‹ï¼š
- **Encoder**: 2å±‚å•å‘LSTM
- **Decoder**: 2å±‚å•å‘LSTM + Attention
- **Attention**: æ”¯æŒ3ç§æ³¨æ„åŠ›æœºåˆ¶
  - `dot`: ç‚¹ç§¯æ³¨æ„åŠ›
  - `general`: é€šç”¨æ³¨æ„åŠ›ï¼ˆLuongï¼‰
  - `concat`: æ‹¼æ¥æ³¨æ„åŠ›ï¼ˆBahdanauï¼‰
- **Seq2Seq**: å®Œæ•´çš„åºåˆ—åˆ°åºåˆ—æ¨¡å‹
  - æ”¯æŒTeacher Forcingå’ŒFree Running
  - æ”¯æŒGreedyå’ŒBeam Searchè§£ç 

#### `models/transformer_nmt.py`
Transformeræ¨¡å‹ï¼š
- **MultiHeadAttention**: å¤šå¤´æ³¨æ„åŠ›
- **PositionwiseFeedForward**: å‰é¦ˆç½‘ç»œ
- **PositionalEncoding**: ä½ç½®ç¼–ç 
- **TransformerEncoder/Decoder**: ç¼–ç å™¨å’Œè§£ç å™¨
- **Transformer**: å®Œæ•´æ¨¡å‹

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### RNNå®éªŒ

#### 1. æ³¨æ„åŠ›æœºåˆ¶å¯¹æ¯”å®éªŒ

**è®­ç»ƒ3ä¸ªæ¨¡å‹ï¼ˆDot, General, Concatï¼‰ï¼š**
```bash
cd /workspace/users/zikun/course_project/final
CUDA_VISIBLE_DEVICES=2 bash run_attention_experiments.sh
```

**è¯„ä¼°ï¼š**
```bash
CUDA_VISIBLE_DEVICES=2 bash run_attention_evaluations.sh
```

#### 4. è¿è¡Œæ‰€æœ‰RNNå®éªŒ

```bash
# è®­ç»ƒæ‰€æœ‰RNNå˜ä½“ï¼ˆæ³¨æ„åŠ›æœºåˆ¶ + è®­ç»ƒç­–ç•¥ï¼‰
CUDA_VISIBLE_DEVICES=2 bash run_all_rnn_experiments.sh

# è¯„ä¼°æ‰€æœ‰æ¨¡å‹
CUDA_VISIBLE_DEVICES=2 bash run_all_evaluations.sh

# ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
python generate_report.py
# è¾“å‡º: results/RNN_EXPERIMENT_REPORT.md
```

---

### Transformerå®éªŒ

#### 1. è®­ç»ƒTransformeræ¨¡å‹

```bash
# é»˜è®¤é…ç½®
CUDA_VISIBLE_DEVICES=2 python train_transformer.py --experiment default --epochs 20 --batch_size 64

# ä¸åŒä½ç½®ç¼–ç 
CUDA_VISIBLE_DEVICES=2 python train_transformer.py --experiment pos_absolute --epochs 20
CUDA_VISIBLE_DEVICES=2 python train_transformer.py --experiment pos_learned --epochs 20

# ä¸åŒæ¨¡å‹å¤§å°
CUDA_VISIBLE_DEVICES=2 python train_transformer.py --experiment small --epochs 20
CUDA_VISIBLE_DEVICES=2 python train_transformer.py --experiment base --epochs 20
CUDA_VISIBLE_DEVICES=2 python train_transformer.py --experiment large --epochs 20
```

#### 2. è¯„ä¼°Transformeræ¨¡å‹

```bash
# Greedy Search
CUDA_VISIBLE_DEVICES=2 python evaluate_transformer.py \
    --checkpoint checkpoints/transformer_nmt/best_model.pt \
    --method greedy \
    --output results/transformer_greedy.json

# Beam Search
CUDA_VISIBLE_DEVICES=2 python evaluate_transformer.py \
    --checkpoint checkpoints/transformer_nmt/best_model.pt \
    --method beam \
    --beam_size 5 \
    --output results/transformer_beam.json
```

---

## ğŸ“Š å®éªŒè®¾ç½®

### RNNå®éªŒ

#### å®éªŒ1: æ³¨æ„åŠ›æœºåˆ¶å¯¹æ¯”
- **Dot-product Attention**: ç®€å•ç‚¹ç§¯
- **General Attention**: å¸¦æƒé‡çŸ©é˜µï¼ˆLuongï¼‰
- **Concat Attention**: æ‹¼æ¥+å‰é¦ˆç½‘ç»œï¼ˆBahdanauï¼‰

#### å®éªŒ2: è®­ç»ƒç­–ç•¥å¯¹æ¯”
- **Teacher Forcing (TF=1.0)**: æ€»æ˜¯ä½¿ç”¨çœŸå®æ ‡ç­¾
- **Free Running (TF=0.0)**: æ€»æ˜¯ä½¿ç”¨æ¨¡å‹é¢„æµ‹
- **Scheduled Sampling (TF=0.5)**: 50%æ¦‚ç‡ä½¿ç”¨çœŸå®æ ‡ç­¾

#### å®éªŒ3: è§£ç ç­–ç•¥å¯¹æ¯”
- **Greedy Search**: æ¯æ­¥é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„è¯
- **Beam Search**: ä¿ç•™top-kå€™é€‰åºåˆ—ï¼ˆk=5ï¼‰

### æ¨¡å‹å‚æ•°

#### RNNæ¨¡å‹
```python
embed_dim = 256          # è¯åµŒå…¥ç»´åº¦
hidden_dim = 512         # éšè—å±‚ç»´åº¦
n_layers = 2             # å±‚æ•°ï¼ˆç¼–ç å™¨å’Œè§£ç å™¨å„2å±‚ï¼‰
dropout = 0.3            # Dropoutç‡
rnn_type = 'LSTM'        # RNNç±»å‹
```

#### Transformeræ¨¡å‹
```python
d_model = 256            # æ¨¡å‹ç»´åº¦
n_heads = 8              # æ³¨æ„åŠ›å¤´æ•°
n_layers = 4             # ç¼–ç å™¨å’Œè§£ç å™¨å±‚æ•°
d_ff = 1024              # å‰é¦ˆç½‘ç»œç»´åº¦
dropout = 0.1            # Dropoutç‡
```

### è®­ç»ƒå‚æ•°
```python
batch_size = 64          # æ‰¹æ¬¡å¤§å°
epochs = 20              # è®­ç»ƒè½®æ•°
learning_rate = 0.001    # å­¦ä¹ ç‡ï¼ˆRNNï¼‰
learning_rate = 0.0001   # å­¦ä¹ ç‡ï¼ˆTransformerï¼‰
optimizer = Adam         # ä¼˜åŒ–å™¨
clip_grad = 1.0          # æ¢¯åº¦è£å‰ª
early_stopping = 5       # æ—©åœpatience
```

---

## ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡

### BLEU Score
- **èŒƒå›´**: 0-1ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
- **è®¡ç®—æ–¹æ³•**: NLTK corpus_bleu with smoothing method 4
- **N-gram**: 1-gram, 2-gram, 3-gram, 4-gram precisions

### è¾“å‡ºç¤ºä¾‹
```
==================================================
BLEU Score: 0.0046
Precision 1-gram: 0.1215
Precision 2-gram: 0.0357
Precision 3-gram: 0.0096
Precision 4-gram: 0.0046
==================================================
```

---

## ğŸ“ è¾“å‡ºæ–‡ä»¶

### è®­ç»ƒè¾“å‡º
- **Checkpoints**: `checkpoints/{experiment_name}/best_model.pt`
  - åŒ…å«ï¼šæ¨¡å‹æƒé‡ã€ä¼˜åŒ–å™¨çŠ¶æ€ã€é…ç½®ã€è®­ç»ƒ/éªŒè¯æŸå¤±
- **æ—¥å¿—**: `logs/train_{experiment_name}.log`
  - æ¯ä¸ªepochçš„è®­ç»ƒå’ŒéªŒè¯æŸå¤±

### è¯„ä¼°è¾“å‡º
- **ç»“æœJSON**: `results/{experiment_name}_{method}.json`
  ```json
  {
    "bleu": 0.0046,
    "precisions": [0.1215, 0.0357, 0.0096, 0.0046],
    "results": [
      {
        "source": "ä¸­æ–‡å¥å­",
        "reference": "å‚è€ƒç¿»è¯‘",
        "hypothesis": "æ¨¡å‹ç¿»è¯‘"
      },
      ...
    ]
  }
  ```

- **å¯¹æ¯”æŠ¥å‘Š**: `results/ATTENTION_REPORT.md` æˆ– `results/RNN_EXPERIMENT_REPORT.md`
  - Markdownæ ¼å¼çš„å®éªŒå¯¹æ¯”è¡¨æ ¼
  - æœ€ä½³æ¨¡å‹æ ‡æ³¨
  - è¯¦ç»†çš„N-gram precisions

---

## ğŸ” ç›‘æ§è®­ç»ƒ

### æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
```bash
# å®æ—¶æŸ¥çœ‹
tail -f logs/train_attention_dot.log

# æŸ¥çœ‹æœ€å50è¡Œ
tail -50 logs/train_attention_dot.log
```

### æ£€æŸ¥GPUä½¿ç”¨
```bash
# å®æ—¶ç›‘æ§
watch -n 1 nvidia-smi

# æŸ¥çœ‹GPU 2
nvidia-smi --id=2
```

### æŸ¥çœ‹å·²è®­ç»ƒæ¨¡å‹
```bash
ls -lh checkpoints/*/best_model.pt
```

---

## âš™ï¸ è‡ªå®šä¹‰å‚æ•°

### ä¿®æ”¹è®­ç»ƒå‚æ•°
```bash
python train_rnn.py \
    --experiment attention_dot \
    --epochs 30 \              # è‡ªå®šä¹‰epochæ•°
    --batch_size 32 \          # è‡ªå®šä¹‰batch size
    --lr 0.0005 \              # è‡ªå®šä¹‰å­¦ä¹ ç‡
    --tf_ratio 0.7             # è‡ªå®šä¹‰teacher forcing ratio
```

### ä¿®æ”¹è¯„ä¼°å‚æ•°
```bash
python evaluate.py \
    --checkpoint checkpoints/rnn_attention_dot/best_model.pt \
    --method beam \
    --beam_size 10 \           # è‡ªå®šä¹‰beam size
    --max_len 150 \            # è‡ªå®šä¹‰æœ€å¤§è¾“å‡ºé•¿åº¦
    --max_samples 500          # åªè¯„ä¼°å‰500ä¸ªæ ·æœ¬
```

---

## ğŸ› å¸¸è§é—®é¢˜

### 1. CUDA Out of Memory
**è§£å†³æ–¹æ¡ˆ**: å‡å°batch size
```bash
python train_rnn.py --experiment default --batch_size 32
```

### 2. è®­ç»ƒé€Ÿåº¦æ…¢
**æ£€æŸ¥**:
- GPUæ˜¯å¦è¢«æ­£ç¡®ä½¿ç”¨ï¼š`nvidia-smi`
- æ˜¯å¦ä½¿ç”¨äº†æ­£ç¡®çš„GPUï¼šæ£€æŸ¥`config.py`ä¸­çš„deviceè®¾ç½®

### 3. BLEUåˆ†æ•°ä¸º0æˆ–å¾ˆä½
**å¯èƒ½åŸå› **:
- æ¨¡å‹è®­ç»ƒä¸å……åˆ†ï¼ˆå¢åŠ epochsï¼‰
- æ•°æ®é‡å¤ªå°ï¼ˆä½¿ç”¨train_100k.jsonlï¼‰
- å­¦ä¹ ç‡ä¸åˆé€‚ï¼ˆè°ƒæ•´lrï¼‰

### 4. è¯æ±‡è¡¨æœªæ‰¾åˆ°
**è§£å†³æ–¹æ¡ˆ**: é¦–æ¬¡è®­ç»ƒä¼šè‡ªåŠ¨ç”Ÿæˆè¯æ±‡è¡¨ï¼Œç¡®ä¿ï¼š
```bash
ls AP0004_Midterm&Final_translation_dataset_zh_en/*.pkl
# åº”è¯¥çœ‹åˆ° src_vocab.pkl å’Œ tgt_vocab.pkl
```

---

## ğŸ“Š å®éªŒæµç¨‹å»ºè®®

### å¿«é€ŸéªŒè¯ï¼ˆ1å°æ—¶ï¼‰
```bash
# 1. è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼ˆ30åˆ†é’Ÿï¼‰
CUDA_VISIBLE_DEVICES=2 python train_rnn.py --experiment attention_dot --epochs 20

# 2. è¯„ä¼°ï¼ˆ5åˆ†é’Ÿï¼‰
CUDA_VISIBLE_DEVICES=2 python evaluate.py \
    --checkpoint checkpoints/rnn_attention_dot/best_model.pt \
    --method greedy

# 3. æŸ¥çœ‹ç»“æœ
cat results/rnn_attention_dot_greedy.json | grep bleu
```

### å®Œæ•´RNNå®éªŒï¼ˆ4-5å°æ—¶ï¼‰
```bash
# 1. è®­ç»ƒæ‰€æœ‰RNNæ¨¡å‹ï¼ˆ3-4å°æ—¶ï¼‰
CUDA_VISIBLE_DEVICES=2 bash run_all_rnn_experiments.sh

# 2. è¯„ä¼°æ‰€æœ‰æ¨¡å‹ï¼ˆ30-45åˆ†é’Ÿï¼‰
CUDA_VISIBLE_DEVICES=2 bash run_all_evaluations.sh

# 3. ç”ŸæˆæŠ¥å‘Š
python generate_report.py

# 4. æŸ¥çœ‹æŠ¥å‘Š
cat results/RNN_EXPERIMENT_REPORT.md
```

### RNN vs Transformerå¯¹æ¯”ï¼ˆ8-10å°æ—¶ï¼‰
```bash
# 1. å®Œæˆæ‰€æœ‰RNNå®éªŒï¼ˆ4-5å°æ—¶ï¼‰
CUDA_VISIBLE_DEVICES=2 bash run_all_rnn_experiments.sh
CUDA_VISIBLE_DEVICES=2 bash run_all_evaluations.sh

# 2. è®­ç»ƒTransformerï¼ˆ2-3å°æ—¶ï¼‰
CUDA_VISIBLE_DEVICES=2 python train_transformer.py --experiment default --epochs 20

# 3. è¯„ä¼°Transformerï¼ˆ15åˆ†é’Ÿï¼‰
CUDA_VISIBLE_DEVICES=2 python evaluate_transformer.py \
    --checkpoint checkpoints/transformer_nmt/best_model.pt \
    --method greedy
CUDA_VISIBLE_DEVICES=2 python evaluate_transformer.py \
    --checkpoint checkpoints/transformer_nmt/best_model.pt \
    --method beam

# 4. å¯¹æ¯”åˆ†æ
# æ‰‹åŠ¨å¯¹æ¯”RNNå’ŒTransformerçš„BLEUåˆ†æ•°
```

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. **Attention Mechanism**:
   - Bahdanau et al. (2015) - Neural Machine Translation by Jointly Learning to Align and Translate
   - Luong et al. (2015) - Effective Approaches to Attention-based Neural Machine Translation

2. **Transformer**:
   - Vaswani et al. (2017) - Attention Is All You Need

3. **Training Strategies**:
   - Bengio et al. (2015) - Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks

4. **BLEU Score**:
   - Papineni et al. (2002) - BLEU: a Method for Automatic Evaluation of Machine Translation
   - Chen and Cherry (2014) - A Systematic Comparison of Smoothing Techniques for Sentence-Level BLEU

---

## ğŸ‘¥ ä½œè€…

è¯¾ç¨‹é¡¹ç›® - Chinese-English Neural Machine Translation

---

## ğŸ“„ è®¸å¯

æœ¬é¡¹ç›®ä»…ç”¨äºå­¦æœ¯ç ”ç©¶å’Œè¯¾ç¨‹ä½œä¸šã€‚

**ç”ŸæˆæŠ¥å‘Šï¼š**
```bash
python generate_attention_report.py
# è¾“å‡º: results/ATTENTION_REPORT.md
```

#### 2. å•ç‹¬è®­ç»ƒæŸä¸ªRNNæ¨¡å‹

```bash
# Dot-product Attention
CUDA_VISIBLE_DEVICES=2 python train_rnn.py --experiment attention_dot --epochs 20 --batch_size 64

# General Attention
CUDA_VISIBLE_DEVICES=2 python train_rnn.py --experiment attention_general --epochs 20 --batch_size 64

# Concat Attention
CUDA_VISIBLE_DEVICES=2 python train_rnn.py --experiment attention_concat --epochs 20 --batch_size 64

# Teacher Forcing
CUDA_VISIBLE_DEVICES=2 python train_rnn.py --experiment teacher_forcing --epochs 20 --batch_size 64

# Free Running
CUDA_VISIBLE_DEVICES=2 python train_rnn.py --experiment free_running --epochs 20 --batch_size 64

# é»˜è®¤é…ç½®ï¼ˆLSTM + General + TF=0.5ï¼‰
CUDA_VISIBLE_DEVICES=2 python train_rnn.py --experiment default --epochs 20 --batch_size 64
```

#### 3. è¯„ä¼°RNNæ¨¡å‹

```bash
# Greedy Search
CUDA_VISIBLE_DEVICES=2 python evaluate.py \
    --checkpoint checkpoints/rnn_attention_dot/best_model.pt \
    --method greedy \
    --output results/rnn_attention_dot_greedy.json

# Beam Search
CUDA_VISIBLE_DEVICES=2 python evaluate.py \
    --checkpoint checkpoints/rnn_attention_dot/best_model.pt \
    --method beam \
    --beam_size 5 \
    --output results/rnn_attention_dot_beam.json
```


